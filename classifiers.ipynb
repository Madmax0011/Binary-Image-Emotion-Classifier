{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.svm import SVC\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T20:06:04.780900Z","iopub.status.busy":"2024-05-17T20:06:04.780321Z","iopub.status.idle":"2024-05-17T20:06:08.027266Z","shell.execute_reply":"2024-05-17T20:06:08.026117Z","shell.execute_reply.started":"2024-05-17T20:06:04.780863Z"},"trusted":true},"outputs":[],"source":["# Load the data\n","training1_data = pd.read_csv(\"/kaggle/input/dataset/data/training1.csv\")\n","training2_data = pd.read_csv(\"/kaggle/input/dataset/data/training2.csv\")\n","test_data = pd.read_csv(\"/kaggle/input/dataset/data/test.csv\")\n","\n","# Separate features, target, and confidence in training data\n","X_train1 = training1_data.drop(columns=['label', 'confidence'])\n","y_train1 = training1_data['label']\n","confidence1 = training1_data['confidence']\n","\n","X_train2 = training2_data.drop(columns=['label', 'confidence'])\n","y_train2 = training2_data['label']\n","confidence2 = training2_data['confidence']"]},{"cell_type":"markdown","metadata":{},"source":["## Details of Dataset\n","The extracted files are:\n","- training1.csv: Contains 400 samples with no missing values.\n","- training2.csv: Contains 2750 samples with some missing values.\n","- test.csv: Contains 1000 samples with some missing values.\n","\n","\n","\n","The training1.csv file contains:\n","- 400 samples (rows).\n","- 3458 columns, including:\n","    - 3072 CNN features (CNNs to CNNs.3071).\n","    - 384 GIST features (GIST.0 to GIST.383).\n","    - A label column indicating the class (1 for happy, 0 for sad).\n","    - A confidence column indicating the confidence of the label.\n","    - No missing values\n","\n","\n","\n","The training2.csv file contains:\n","- 2750 samples (rows).\n","- 3458 columns, including:\n","     - 3072 CNN features (CNNs to CNNs.3071).\n","     - 384 GIST features (GIST.0 to GIST.383).\n","     - A label column indicating the class (1 for happy, 0 for sad).\n","     - A confidence column indicating the confidence of the label.\n","     - This dataset has missing values (NaNs) in the feature columns.\n","\n","\n","The test.csv file contains:\n","- 1000 samples (rows).\n","- 3456 columns, including:\n","     - 3072 CNN features (CNNs to CNNs.3071).\n","     - 384 GIST features (GIST.0 to GIST.383).\n","     - This dataset also has missing values (NaNs) in the feature columns."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T20:20:29.715599Z","iopub.status.busy":"2024-05-16T20:20:29.714811Z","iopub.status.idle":"2024-05-16T20:20:29.741243Z","shell.execute_reply":"2024-05-16T20:20:29.740069Z","shell.execute_reply.started":"2024-05-16T20:20:29.715562Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CNNs</th>\n","      <th>CNNs.1</th>\n","      <th>CNNs.2</th>\n","      <th>CNNs.3</th>\n","      <th>CNNs.4</th>\n","      <th>CNNs.5</th>\n","      <th>CNNs.6</th>\n","      <th>CNNs.7</th>\n","      <th>CNNs.8</th>\n","      <th>CNNs.9</th>\n","      <th>...</th>\n","      <th>GIST.374</th>\n","      <th>GIST.375</th>\n","      <th>GIST.376</th>\n","      <th>GIST.377</th>\n","      <th>GIST.378</th>\n","      <th>GIST.379</th>\n","      <th>GIST.380</th>\n","      <th>GIST.381</th>\n","      <th>GIST.382</th>\n","      <th>GIST.383</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00</td>\n","      <td>0.00000</td>\n","      <td>0.33607</td>\n","      <td>1.58840</td>\n","      <td>...</td>\n","      <td>0.007640</td>\n","      <td>NaN</td>\n","      <td>0.036742</td>\n","      <td>0.012381</td>\n","      <td>NaN</td>\n","      <td>0.053308</td>\n","      <td>0.026501</td>\n","      <td>0.005391</td>\n","      <td>0.001272</td>\n","      <td>0.001446</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.00</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>2.25540</td>\n","      <td>...</td>\n","      <td>0.040871</td>\n","      <td>0.020330</td>\n","      <td>0.043143</td>\n","      <td>0.019345</td>\n","      <td>0.016736</td>\n","      <td>0.008209</td>\n","      <td>0.023059</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.022575</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.00000</td>\n","      <td>0.080498</td>\n","      <td>NaN</td>\n","      <td>0.00000</td>\n","      <td>0.00</td>\n","      <td>NaN</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.035165</td>\n","      <td>0.027588</td>\n","      <td>0.039189</td>\n","      <td>0.027310</td>\n","      <td>0.038010</td>\n","      <td>0.003747</td>\n","      <td>0.016547</td>\n","      <td>NaN</td>\n","      <td>0.017964</td>\n","      <td>0.034397</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.39567</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.00</td>\n","      <td>NaN</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.049510</td>\n","      <td>0.027773</td>\n","      <td>0.020592</td>\n","      <td>0.044585</td>\n","      <td>0.032217</td>\n","      <td>0.054913</td>\n","      <td>0.035068</td>\n","      <td>0.021064</td>\n","      <td>0.020542</td>\n","      <td>0.033792</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.037334</td>\n","      <td>0.0</td>\n","      <td>0.90437</td>\n","      <td>1.17</td>\n","      <td>0.40552</td>\n","      <td>0.00000</td>\n","      <td>0.21256</td>\n","      <td>...</td>\n","      <td>0.003357</td>\n","      <td>0.021205</td>\n","      <td>0.003779</td>\n","      <td>0.006411</td>\n","      <td>NaN</td>\n","      <td>0.003991</td>\n","      <td>0.012906</td>\n","      <td>0.008374</td>\n","      <td>0.002190</td>\n","      <td>0.042025</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 3456 columns</p>\n","</div>"],"text/plain":["   CNNs  CNNs.1   CNNs.2    CNNs.3  CNNs.4   CNNs.5  CNNs.6   CNNs.7   CNNs.8  \\\n","0   0.0     0.0  0.00000       NaN     0.0  0.00000    0.00  0.00000  0.33607   \n","1   0.0     0.0  0.00000  0.000000     0.0      NaN    0.00  0.00000  0.00000   \n","2   0.0     NaN  0.00000  0.080498     NaN  0.00000    0.00      NaN  0.00000   \n","3   0.0     0.0  0.39567  0.000000     0.0  0.00000    0.00      NaN  0.00000   \n","4   NaN     NaN      NaN  0.037334     0.0  0.90437    1.17  0.40552  0.00000   \n","\n","    CNNs.9  ...  GIST.374  GIST.375  GIST.376  GIST.377  GIST.378  GIST.379  \\\n","0  1.58840  ...  0.007640       NaN  0.036742  0.012381       NaN  0.053308   \n","1  2.25540  ...  0.040871  0.020330  0.043143  0.019345  0.016736  0.008209   \n","2  0.00000  ...  0.035165  0.027588  0.039189  0.027310  0.038010  0.003747   \n","3  0.00000  ...  0.049510  0.027773  0.020592  0.044585  0.032217  0.054913   \n","4  0.21256  ...  0.003357  0.021205  0.003779  0.006411       NaN  0.003991   \n","\n","   GIST.380  GIST.381  GIST.382  GIST.383  \n","0  0.026501  0.005391  0.001272  0.001446  \n","1  0.023059       NaN       NaN  0.022575  \n","2  0.016547       NaN  0.017964  0.034397  \n","3  0.035068  0.021064  0.020542  0.033792  \n","4  0.012906  0.008374  0.002190  0.042025  \n","\n","[5 rows x 3456 columns]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["X_train2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T17:52:03.351073Z","iopub.status.busy":"2024-05-16T17:52:03.350385Z","iopub.status.idle":"2024-05-16T17:52:03.561770Z","shell.execute_reply":"2024-05-16T17:52:03.560738Z","shell.execute_reply.started":"2024-05-16T17:52:03.351042Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 400 entries, 0 to 399\n","Columns: 3456 entries, CNNs to GIST.383\n","dtypes: float64(3456)\n","memory usage: 10.5 MB\n"]}],"source":["X_train1.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T20:20:11.925554Z","iopub.status.busy":"2024-05-16T20:20:11.924622Z","iopub.status.idle":"2024-05-16T20:20:11.950902Z","shell.execute_reply":"2024-05-16T20:20:11.949712Z","shell.execute_reply.started":"2024-05-16T20:20:11.925503Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CNNs</th>\n","      <th>CNNs.1</th>\n","      <th>CNNs.2</th>\n","      <th>CNNs.3</th>\n","      <th>CNNs.4</th>\n","      <th>CNNs.5</th>\n","      <th>CNNs.6</th>\n","      <th>CNNs.7</th>\n","      <th>CNNs.8</th>\n","      <th>CNNs.9</th>\n","      <th>...</th>\n","      <th>GIST.374</th>\n","      <th>GIST.375</th>\n","      <th>GIST.376</th>\n","      <th>GIST.377</th>\n","      <th>GIST.378</th>\n","      <th>GIST.379</th>\n","      <th>GIST.380</th>\n","      <th>GIST.381</th>\n","      <th>GIST.382</th>\n","      <th>GIST.383</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>0.2334</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.79188</td>\n","      <td>0.0000</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>0.43900</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.009773</td>\n","      <td>NaN</td>\n","      <td>0.011548</td>\n","      <td>NaN</td>\n","      <td>0.017014</td>\n","      <td>NaN</td>\n","      <td>0.020395</td>\n","      <td>NaN</td>\n","      <td>0.007909</td>\n","      <td>0.024576</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.45386</td>\n","      <td>0.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.00000</td>\n","      <td>1.1775</td>\n","      <td>0.0</td>\n","      <td>0.0000</td>\n","      <td>0.42297</td>\n","      <td>2.02510</td>\n","      <td>...</td>\n","      <td>0.009820</td>\n","      <td>0.026096</td>\n","      <td>0.039678</td>\n","      <td>NaN</td>\n","      <td>0.057236</td>\n","      <td>0.023440</td>\n","      <td>NaN</td>\n","      <td>0.014737</td>\n","      <td>0.013860</td>\n","      <td>0.058389</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00000</td>\n","      <td>0.0000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>0.0000</td>\n","      <td>0.0</td>\n","      <td>0.0000</td>\n","      <td>0.00000</td>\n","      <td>0.31986</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.026954</td>\n","      <td>0.050490</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.021365</td>\n","      <td>0.027606</td>\n","      <td>0.031131</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.22014</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.88192</td>\n","      <td>1.0936</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.007899</td>\n","      <td>0.023398</td>\n","      <td>NaN</td>\n","      <td>0.022786</td>\n","      <td>NaN</td>\n","      <td>0.007288</td>\n","      <td>0.043885</td>\n","      <td>NaN</td>\n","      <td>0.011621</td>\n","      <td>0.022733</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.00000</td>\n","      <td>0.0000</td>\n","      <td>0.0</td>\n","      <td>1.7938</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.012921</td>\n","      <td>NaN</td>\n","      <td>0.019792</td>\n","      <td>0.019010</td>\n","      <td>0.003771</td>\n","      <td>0.003214</td>\n","      <td>0.001543</td>\n","      <td>NaN</td>\n","      <td>0.003199</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 3456 columns</p>\n","</div>"],"text/plain":["      CNNs  CNNs.1  CNNs.2  CNNs.3   CNNs.4  CNNs.5  CNNs.6  CNNs.7   CNNs.8  \\\n","0      NaN  0.2334     0.0     NaN  0.79188  0.0000     NaN  0.0000  0.43900   \n","1  0.45386  0.0000     NaN     NaN  0.00000  1.1775     0.0  0.0000  0.42297   \n","2  0.00000  0.0000     0.0     0.0  0.00000  0.0000     0.0  0.0000  0.00000   \n","3  0.22014     NaN     0.0     NaN  0.88192  1.0936     NaN  0.0000  0.00000   \n","4      NaN  0.0000     NaN     NaN  0.00000  0.0000     0.0  1.7938  0.00000   \n","\n","    CNNs.9  ...  GIST.374  GIST.375  GIST.376  GIST.377  GIST.378  GIST.379  \\\n","0  0.00000  ...  0.009773       NaN  0.011548       NaN  0.017014       NaN   \n","1  2.02510  ...  0.009820  0.026096  0.039678       NaN  0.057236  0.023440   \n","2  0.31986  ...       NaN       NaN  0.026954  0.050490       NaN       NaN   \n","3  0.00000  ...  0.007899  0.023398       NaN  0.022786       NaN  0.007288   \n","4  0.00000  ...  0.012921       NaN  0.019792  0.019010  0.003771  0.003214   \n","\n","   GIST.380  GIST.381  GIST.382  GIST.383  \n","0  0.020395       NaN  0.007909  0.024576  \n","1       NaN  0.014737  0.013860  0.058389  \n","2       NaN  0.021365  0.027606  0.031131  \n","3  0.043885       NaN  0.011621  0.022733  \n","4  0.001543       NaN  0.003199       NaN  \n","\n","[5 rows x 3456 columns]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["test_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the ranges for CNN and GIST features\n","cnn_feature_range = range(3072)\n","gist_feature_range = range(3072, 3456)\n","\n","# Calculate total values and null values for CNN features\n","cnn_total_values = X_train2.iloc[:, cnn_feature_range].size\n","cnn_null_values = X_train2.iloc[:, cnn_feature_range].isnull().sum().sum()\n","\n","# Calculate total values and null values for GIST features\n","gist_total_values = X_train2.iloc[:, gist_feature_range].size\n","gist_null_values = X_train2.iloc[:, gist_feature_range].isnull().sum().sum()\n","\n","# Calculate total values and null values for all features\n","total_values = cnn_total_values + gist_total_values\n","total_null_values = cnn_null_values + gist_null_values\n","\n","# Calculate the percentage of null values\n","cnn_null_percentage = (cnn_null_values / cnn_total_values) * 100\n","gist_null_percentage = (gist_null_values / gist_total_values) * 100\n","total_null_percentage = (total_null_values / total_values) * 100\n","\n","print(f\"Train Data\")\n","print(f\"Total Data values: {total_values}\")\n","print(f\"Total null values: {total_null_values}\")\n","print(f\"Count of null values in CNN features: {cnn_null_values}\")\n","print(f\"Count of null values in GIST features: {gist_null_values}\")\n","print(f\"percentage of null values with total data values: {total_null_percentage:.2f}\\n\")\n","\n","# Define the ranges for CNN and GIST features\n","cnn_feature_range = range(3072)\n","gist_feature_range = range(3072, 3456)\n","\n","# Calculate total values and null values for CNN features\n","cnn_total_values = test_data.iloc[:, cnn_feature_range].size\n","cnn_null_values = test_data.iloc[:, cnn_feature_range].isnull().sum().sum()\n","\n","# Calculate total values and null values for GIST features\n","gist_total_values = test_data.iloc[:, gist_feature_range].size\n","gist_null_values = test_data.iloc[:, gist_feature_range].isnull().sum().sum()\n","\n","# Calculate total values and null values for all features\n","total_values = cnn_total_values + gist_total_values\n","total_null_values = cnn_null_values + gist_null_values\n","\n","# Calculate the percentage of null values\n","cnn_null_percentage = (cnn_null_values / cnn_total_values) * 100\n","gist_null_percentage = (gist_null_values / gist_total_values) * 100\n","total_null_percentage = (total_null_values / total_values) * 100\n","\n","print(f\"Test Data\")\n","print(f\"Total Data values: {total_values}\")\n","print(f\"Total null values: {total_null_values}\")\n","print(f\"Count of null values in CNN features: {cnn_null_values}\")\n","print(f\"Count of null values in GIST features: {gist_null_values}\")\n","print(f\"percentage of null values with total data values: {total_null_percentage:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the ranges for CNN and GIST features\n","cnn_feature_range = range(3072)\n","gist_feature_range = range(3072, 3456)\n","\n","# Calculate total values and null values for CNN features\n","cnn_total_values = X_train2.iloc[:, cnn_feature_range].size\n","cnn_null_values = X_train2.iloc[:, cnn_feature_range].isnull().sum().sum()\n","\n","# Calculate total values and null values for GIST features\n","gist_total_values = X_train2.iloc[:, gist_feature_range].size\n","gist_null_values = X_train2.iloc[:, gist_feature_range].isnull().sum().sum()\n","\n","# Calculate total values and null values for all features\n","total_values = cnn_total_values + gist_total_values\n","total_null_values = cnn_null_values + gist_null_values\n","\n","# Create a DataFrame for visualization\n","null_counts_df = pd.DataFrame({\n","    'Feature Type': ['CNN Features', 'GIST Features', 'Total Null Values', 'Total Data Values'],\n","    'Values': [cnn_null_values, gist_null_values, total_null_values, total_values - total_null_values],\n","    'Total Values': [cnn_total_values, gist_total_values, total_values, total_values]\n","})\n","\n","# Plot the null values\n","plt.figure(figsize=(12, 8))\n","bars = plt.bar(null_counts_df['Feature Type'], null_counts_df['Values'], color=['blue', 'green', 'red', 'purple'])\n","\n","\n","plt.title('Null Values and Total Data Values in CNN and GIST Features (Training Data)')\n","plt.ylabel('Count of Values')\n","plt.xlabel('Feature Type')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Count total null values\n","total_null_counts_train = cnn_null_values + gist_null_values\n","\n","# Create a DataFrame for visualization\n","null_counts_df = pd.DataFrame({\n","    'Feature Type': ['CNN Features', 'GIST Features', 'Total Null Values'],\n","    'Null Values': [cnn_null_values, gist_null_values, total_null_counts_train]\n","})\n","\n","# Plot the null values\n","plt.figure(figsize=(10, 6))\n","plt.bar(null_counts_df['Feature Type'], null_counts_df['Null Values'], color=['blue', 'green', 'red'])\n","plt.title('Null Values in CNN and GIST Features (Training Data)')\n","plt.ylabel('Count of Null Values')\n","plt.xlabel('Feature Type')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine training datasets\n","X_train_combined = pd.concat([X_train1, X_train2], axis=0)\n","y_train_combined = pd.concat([y_train1, y_train2], axis=0)\n","confidence_combined = pd.concat([confidence1, confidence2], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Handle missing values\n","imputer = SimpleImputer(strategy='constant',fill_value=0)\n","X_train_imputed = imputer.fit_transform(X_train_combined)\n","X_test_imputed = imputer.transform(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["## Scaling Data"]},{"cell_type":"markdown","metadata":{},"source":["StandardScaler standardizes the features by removing the mean and scaling to unit variance. This means each feature will have a mean of 0 and a standard deviation of 1. This is achieved by:\n","\n","$$\n","X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n","$$\n"," \n","where ùúá is the mean of the feature and ùúé is the standard deviation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Rescale the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Selection"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluated three different models: \n","    - Logistic Regression\n","    - Random Forest\n","    - Support Vector Machine (SVM).\n","- Cross-validation (with 5 folds) was used to assess the performance of each model.\n","- For each model, Calculated the mean accuracy across all folds.\n","- Plotted the accuracy for each fold and the mean accuracy for visual comparison.\n","- This approach helped identify the model with the best overall performance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the combined training data\n","X_train, X_val, y_train, y_val, confidence_train, confidence_val = train_test_split(\n","    X_train_scaled, y_train_combined, confidence_combined, test_size=0.2, random_state=42)\n","\n","# Define classifiers\n","classifiers = {\n","    'Logistic Regression': LogisticRegression(max_iter=1000),\n","    'Random Forest': RandomForestClassifier(),\n","    'SVM': SVC()\n","}\n","\n","# Evaluate classifiers using cross-validation with sample weights\n","results = {}\n","for clf_name, clf in classifiers.items():\n","    scores = cross_val_score(clf, X_train_scaled, y_train_combined, cv=5, scoring='accuracy', fit_params={'sample_weight': confidence_combined})\n","    results[clf_name] = scores\n","    # Plot the cross-validation results\n","    title = 'Cross-Validation Accuracy Scores of '+clf_name\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(range(1, 6), scores, color='blue', label='Fold Accuracy')\n","    plt.scatter([6], [np.mean(scores)], color='red', label='Mean Accuracy')\n","    plt.axhline(np.mean(scores), color='red', linestyle='--')\n","    plt.title(title)\n","    plt.xlabel('Fold')\n","    plt.ylabel('Accuracy')\n","    plt.xticks(range(1, 7), ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5', 'Mean'])\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","    print(f\"scores: {scores}\\n {clf_name} Accuracy: {scores.mean():.4f} ¬± {scores.std():.4f}\")\n","\n","# Convert results to DataFrame for visualization\n","results_df = pd.DataFrame(results)\n","\n","# Plot the comparison of different models\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(data=results_df, palette=\"Set2\")\n","plt.title('Comparison of Model Performance')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Model')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["- **Cross-Validation Analysis**\n","     - Tried 5 different cross-validation (CV) values for each model.\n","     - Recorded the mean accuracy for each CV value.\n","     - Plotted a line graph for each model with mean accuracy values as markers on the lines to visualize performance trends across different CV values."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","X_train, X_val, y_train, y_val, confidence_train, confidence_val = train_test_split(\n","    X_train_scaled, y_train_combined, confidence_combined, test_size=0.2, random_state=42)\n","\n","# Define classifiers with increased max_iter for Logistic Regression and MLP\n","classifiers = {\n","    'Logistic Regression': LogisticRegression(max_iter=1000),\n","    'Random Forest': RandomForestClassifier(),\n","    'SVM': SVC(),\n","}\n","\n","# Perform cross-validation 5 times for each model and store the mean accuracies\n","mean_accuracies = {name: [] for name in classifiers.keys()}\n","\n","for name, clf in classifiers.items():\n","    for i in range(5):\n","        scores = cross_val_score(clf, X_train, y_train, cv=3+i, scoring='accuracy')\n","        mean_accuracies[name].append(np.mean(scores))\n","\n","# Plot the mean accuracies for each model\n","plt.figure(figsize=(12, 8))\n","for name, accuracies in mean_accuracies.items():\n","    plt.plot(range(1, 6), accuracies, marker='o', label=name)\n","\n","plt.title('Cross-Validation Mean Accuracy Scores for Different Models')\n","plt.xlabel('Validation Iteration')\n","plt.ylabel('Mean Accuracy')\n","plt.xticks(range(1, 6))\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Display the mean accuracies\n","for name, accuracies in mean_accuracies.items():\n","    print(f\"{name}: {accuracies}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["- **Random Forest:** \n","    - Evaluated with different numbers of trees (10, 50, 100, 150, 200) using 5-fold cross-validation, showing improved performance with more trees and providing valuable feature importance insights."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the range of number of trees to test\n","n_estimators_range = [10, 50, 100, 150, 200]\n","\n","# Initialize lists to store results\n","train_scores = []\n","test_scores = []\n","\n","# Evaluate Random Forest with different numbers of trees\n","for n in n_estimators_range:\n","    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n","    train_score = np.mean(cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy', fit_params={'sample_weight': confidence_train}))\n","    test_score = rf.fit(X_train, y_train, sample_weight=confidence_train).score(X_val, y_val)\n","    train_scores.append(train_score)\n","    test_scores.append(test_score)\n","\n","# Plot the results\n","plt.figure(figsize=(10, 6))\n","plt.plot(n_estimators_range, train_scores, label=\"Training score\", marker='o')\n","plt.plot(n_estimators_range, test_scores, label=\"Validation score\", marker='o')\n","plt.title('Random Forest Performance with Different Numbers of Trees')\n","plt.xlabel('Number of Trees')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Imputation and Model Training with Different Combinations"]},{"cell_type":"markdown","metadata":{},"source":["- Function Definition:\n","    - Trainig Data 2 is used for Training and then evalution was done on Training Data 1.\n","    - The `train_and_evaluate` function handles the imputation, scaling,splitting data, model training, and evaluation.\n","    - The function parameters allows to specify the imputation strategy, fill value, and whether to use sample weights.\n","        - Imputation:\n","            - **Mean Imputation:** Missing values are replaced with the mean of each column.\n","            - **Constant Value Imputation:** Missing values are replaced with a specified constant value (e.g., 0).\n","        - Scaling:\n","            - The data is scaled to have a mean of 0 and a standard deviation of 1 using StandardScaler.\n","        - Training and Evaluation:\n","\n","            - **Model Training:** An SVM model is trained with and without sample weights.\n","            - **Prediction and Evaluation:** The model is used to predict on the validation data, and the classification report is printed.\n","- Scenario Execution:\n","\n","    - The code defines a list of scenarios to be executed.\n","    - For each scenario, the `train_and_evaluate` function is called with the appropriate parameters, and the results are printed."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_and_evaluate(strategy, fill_value=None, use_sample_weight=True):\n","    if strategy == 'constant':\n","        imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n","    else:\n","        imputer = SimpleImputer(strategy=strategy)\n","\n","    # Impute the missing values\n","    X_train_imputed = imputer.fit_transform(X_train2)\n","    X_test_imputed = imputer.transform(X_train1)\n","\n","    # Rescale the data\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train_imputed)\n","    X_test_scaled = scaler.transform(X_test_imputed)\n","    \n","    X_train, X_val, y_train, y_val, confidence_train, confidence_val = X_train_scaled,X_test_scaled,y_train2,y_train1,confidence2,confidence1\n","\n","    # Train the model\n","    best_clf = SVC()\n","    if use_sample_weight:\n","        best_clf.fit(X_train_scaled, y_train2, sample_weight=confidence_train)\n","    else:\n","        best_clf.fit(X_train_scaled, y_train2)\n","\n","    # Predict on the validation data\n","    val_predictions = best_clf.predict(X_test_scaled)\n","    print(f\"Strategy: {strategy}, Fill Value: {fill_value}, Use Sample Weight: {use_sample_weight}\")\n","    print(classification_report(y_train1, val_predictions),\"\\n\")\n","\n","# Execute the scenarios\n","scenarios = [\n","    ('mean', None, True),\n","    ('mean', None, False),\n","    ('constant', 0, True),\n","    ('constant', 0, False)\n","]\n","\n","for strategy, fill_value, use_sample_weight in scenarios:\n","    train_and_evaluate(strategy, fill_value, use_sample_weight)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Importance\n","- The dataset includes two types of features: CNN features (3072 features) and GIST features (384 features).\n","- We used the Random Forest classifier to analyze feature importance because it provides inherent feature importance scores.\n","- Random Forests are ensemble methods that create multiple decision trees during training and output the mean prediction of individual trees. They are capable of ranking the importance of features.\n","\n","- **Procedure:**\n","\n","    - Train the Random Forest model on the combined dataset.\n","    - Extract feature importance scores from the trained model.\n","    - Aggregate the importance scores for CNN features and GIST features separately."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the combined training data\n","X_train, X_val, y_train, y_val, confidence_train, confidence_val = train_test_split(\n","    X_train_scaled, y_train_combined, confidence_combined, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest model\n","rf = RandomForestClassifier(n_estimators=300,random_state=42)\n","rf.fit(X_train, y_train, sample_weight=confidence_train)\n","\n","# Get feature importance from the Random Forest model\n","feature_importances = rf.feature_importances_\n","\n","# Define the feature ranges for CNN and GIST\n","cnn_feature_range = range(3072)\n","gist_feature_range = range(3072, 3456)\n","\n","# Aggregate feature importances for CNN and GIST features\n","cnn_importance = np.sum(feature_importances[cnn_feature_range])\n","gist_importance = np.sum(feature_importances[gist_feature_range])\n","\n","# Display the aggregated feature importances\n","print(f\"Aggregated feature importance for CNN features: {cnn_importance}\")\n","print(f\"Aggregated feature importance for GIST features: {gist_importance}\")\n","\n","# Plot the feature importances\n","plt.figure(figsize=(10, 6))\n","plt.bar(['CNN Features', 'GIST Features'], [cnn_importance, gist_importance], color=['blue', 'green'])\n","plt.title('Aggregated Feature Importance for CNN and GIST Features')\n","plt.ylabel('Importance')\n","plt.xlabel('Feature Type')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training for prediction"]},{"cell_type":"markdown","metadata":{},"source":["- Best Performing Model:\n","    - After trying out different models with various parameter combinations, SVM emerged as the best-performing model.\n","    \n","- Handling Missing Data:\n","     - Missing data was imputed using a constant value of 0.\n","     \n","- Use of Sample Weights:\n","    - Sample weights were utilized during model training to account for the confidence levels in the training labels.\n","    \n","    \n","In summary, the SVM model with constant value imputation for missing data and the use of sample weights provided the best performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["imputer = SimpleImputer(strategy='constant',fill_value=0)\n","X_train_imputed = imputer.fit_transform(X_train_combined)\n","X_test_imputed = imputer.transform(test_data)\n","\n","# Rescale the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_imputed)\n","X_test_scaled = scaler.transform(X_test_imputed)\n","\n","X_train, X_val, y_train, y_val, confidence_train, confidence_val = train_test_split(\n","    X_train_scaled, y_train_combined, confidence_combined, test_size=0.1, random_state=42)\n","\n","# Train the best model on the entire training data with sample weights\n","best_clf = RandomForestClassifier(random_state=42)\n","best_clf.fit(X_train, y_train, sample_weight=confidence_train)\n","\n","# Predict on the test data\n","predictions = best_clf.predict(X_val)\n","print(classification_report(y_val, predictions))\n","\n","# Save the predictions\n","test_predictions = best_clf.predict(X_test_imputed)\n","predictions_df = pd.DataFrame({'prediction': test_predictions})\n","predictions_df.to_csv('predictions.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5008990,"sourceId":8415247,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
